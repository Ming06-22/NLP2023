{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x5HYyQTg7qpCs1d8TSFFCE9RFMX3CnGU","timestamp":1678085670815},{"file_id":"1wgo33YMqyTmwPXBCgDYvDD39Hgz516zV","timestamp":1677946745923},{"file_id":"1ZNQQshRjVp-0vLNi6ZGRtXX102EWHRq4","timestamp":1598302241860},{"file_id":"1XOa--UHuAQpBRcdqYbFcb8QuUTvywsSk","timestamp":1568522504552},{"file_id":"1LShMg_-e2SzrjDMxSgVbnYyTAgwcJov0","timestamp":1568420694683}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SgNZTjrhcHa0"},"source":["## Lab#1, NLP Spring 2023\n","\n","### This is due on 2023/03/06 15:30, commit to your github as a PDF (lab1.pdf) (File>Print>Save as PDF). \n","\n","#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this lab!\n","\n","***LINK: *paste your link here****\n","####https://colab.research.google.com/drive/1LaDTR5Idye2YSOhxbn3AuzxfXehQN9bw?usp=sharing\n","---"]},{"cell_type":"markdown","metadata":{"id":"hpg8eU9wNBci"},"source":["**Student ID**: B0928007\n","\n","**Name**: 余明昌\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r1_cCBOwfPxI"},"source":["##Question 1 (100 points)\n","Let's switch over to coding! Write some code in this cell to compute the number of unique word **tokens** in this paragraph (5 steps of Text Normalisation: 1. Lowercase Conversion, 2. Remove punctuations, 3. Stemming, 4. Lemmatisation, 5. Stopword Removal). Use a whitespace tokenizer to separate words (i.e., split the string by white space). Be sure that the cell's output is visible in the PDF file you turn in on Github.\n","\n","---\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"STVNzF0frsX8"}},{"cell_type":"code","metadata":{"id":"W9Fm6AQJQDFa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678088881255,"user_tz":-480,"elapsed":461,"user":{"displayName":"余明昌","userId":"16390494860865880575"}},"outputId":"1c768973-011a-4683-c600-cd0e642373c0"},"source":["paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n","that I was passing through the iron gates that led to the driveway.\n","The drive was just a narrow track now, its stony surface covered\n","with grass and weeds. Sometimes, when I thought I had lost it, it\n","would appear again, beneath a fallen tree or beyond a muddy pool \n","formed by the winter rains. The trees had thrown out new\n","low branches which stretched across my way. I came to the house\n","suddenly, and stood there with my heart beating fast and tears\n","filling my eyes.'''\n","\n","# DO NOT MODIFY THE VARIABLES\n","tokens = 0\n","word_tokens = []\n","\n","# YOUR CODE HERE! POPULATE THE tokens and word_tokens VARIABLES WITH THE CORRECT VALUES!\n","# split the words\n","import re\n","word_tokens = re.split(\" |\\n\", paragraph)\n","\n","# Lowercase Conversion\n","word_tokens = [w.lower() for w in word_tokens]\n","\n","# Stopword removal\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download(\"stopwords\")\n","stop_words = set(stopwords.words(\"english\"))\n","word_tokens = [word for word in word_tokens if word not in stop_words]\n","\n","# Remove punctuation\n","def remove_punctuation(word):\n","  return [word for word in word_tokens if word.isalpha()]\n","word_tokens = remove_punctuation(word_tokens)\n","\n","# Stemming\n","from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n","port = PorterStemmer()\n","# word_tokens = [port.stem(token) for token in word_tokens]\n","\n","lanc = LancasterStemmer()\n","# word_tokens = [lanc.stem(token) for token in word_tokens]\n","\n","snow = SnowballStemmer(\"english\")\n","word_tokens = [snow.stem(token) for token in word_tokens]\n","\n","# Lemmatisation\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatiser = WordNetLemmatizer()\n","word_tokens = [lemmatiser.lemmatize(token) for token in word_tokens]\n","\n","# assign value to tokens\n","tokens = len(word_tokens)\n","\n","# DO NOT MODIFY THE BELOW LINE!\n","print('Number of word tokens: %d' % (tokens))\n","print(\"printing lists separated by commas\")\n","print(*word_tokens, sep = \", \") "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of word tokens: 44\n","printing lists separated by commas\n","last, night, dream, went, manderley, seem, pas, iron, gate, led, drive, narrow, track, stoni, surfac, cover, grass, thought, lost, would, appear, beneath, fallen, tree, beyond, muddi, pool, form, winter, tree, thrown, new, low, branch, stretch, across, came, hous, stood, heart, beat, fast, tear, fill\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]}]}